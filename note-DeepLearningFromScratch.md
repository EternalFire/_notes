
# note-深度学习入门

- [note-深度学习入门](#note-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8)
  - [1. Python入门](#1-Python%E5%85%A5%E9%97%A8)
  - [2. 感知机](#2-%E6%84%9F%E7%9F%A5%E6%9C%BA)
  - [3. 神经网络](#3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)
    - [激活函数](#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0)
      - [阶跃函数](#%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0)
      - [sigmoid 函数](#sigmoid-%E5%87%BD%E6%95%B0)
      - [ReLU 函数](#ReLU-%E5%87%BD%E6%95%B0)
    - [多维数组](#%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84)
    - [softmax 函数](#softmax-%E5%87%BD%E6%95%B0)
    - [输出层神经元的数量](#%E8%BE%93%E5%87%BA%E5%B1%82%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E6%95%B0%E9%87%8F)
  - [4. 神经网络的学习](#4-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0)
    - [从数据中学习](#%E4%BB%8E%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%AD%A6%E4%B9%A0)
    - [损失函数](#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0)
    - [mini-batch](#mini-batch)

---

## 1. Python入门

对应元素的乘法, element-wise product

向量: 一维数组

矩阵: 二维数组

张量(tensor): 多维数组

使用 matplotlib 绘制函数图形

---

## 2. 感知机

感知机(perceptron)

> 感知机接收多个输入信号，输出一个信号。


输入信号的加权总和 超过 阈值, 激活神经元:

信号($x_i$) * 权重($w_i$) > 阈值($\theta$), 输出信号 1 (表示神经元被激活).


> 权重相当于电流里所说的电阻.

> 学习是确定 合适的参数的过程，而人要做的是思考感知机的构造(模型)，并把 训练数据交给计算机。

=> 需要确认的参数: 权重, 阈值

感知机的数学表达:

$$
y = \begin{cases}
    1, & \sum x_i w_i + b \gt 0 \\
    0, & \sum x_i w_i + b \le 0 \\
\end{cases}
$$

b, 偏置, $- \theta$

权重, 控制输入信号的重要性

偏置, 控制神经元被激活的容易程度

> 若 b 为 −0.1，则只要输入信号的加权总和超过 0.1，神经元就会被激活。但是如果 b 为 −20.0，则输入信号的加权总和必须超过 20.0，神经元才会被激活

线性空间, 由直线分割的空间; 非线性空间, 由曲线分割的空间.


---

## 3. 神经网络

输入层, 隐藏层, 输出层

### 激活函数

#### 阶跃函数

神经元内有 激活函数(activation function), `h()`.

$$
y = h(b + w_1x_1 + w_2x_2)
$$

$$
a = b + w_1x_1 + w_2x_2
$$

$$
y = h(a)
$$


> 以阈值为界，一旦输入超过阈值，就切换输出。这样的函数称为“阶跃函数”

阶跃函数:

$$
y =
\begin{cases}
1, & x \gt 0 \\
0, & x \le 0
\end{cases}
$$

阶跃函数计算的结果呈阶梯式变化, 值域 $\{ 0, 1 \}$.

#### sigmoid 函数

sigmoid 函数, 值域 $[0, 1]$:

$$
h(x) = \frac{1}{1 + e^{-x}}
$$

e, 自然对数函数的底数, 欧拉数, 纳皮尔常数.

函数是转换器.

> sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化。

阶跃函数 和 sigmoid函数 输出的值都在 $[0, 1]$ 内, 都是非线性函数(函数图像不是直线).

> 为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。


#### ReLU 函数

ReLU(Rectified Linear Unit) 函数:

$$
h(x) =
\begin{cases}
x, & x \gt 0\\
0, & x \le 0
\end{cases}
$$

> ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0

### 多维数组

`np.maximum(0, x)`

> 数组的维数可以通过 np.ndim() 函数获得

`np.dot(A, B)`, 矩阵乘法 / 多维数组点积, 需要满足: `A.shape[1] == B.shape[0]`

[ [a, b], [c, d] ]

第0维数据(行): [a,b], [c,d]

第1维数据(列):  a,b,   c,d

神经网络的前向处理(从输入到输出)

$$
\text{X W = Y}
$$

$$
    \begin{pmatrix}
    x_1 & x_2 \\
    \end{pmatrix}

    \begin{pmatrix}
    w_1 & w_3 & w_5 \\
    w_2 & w_4 & w_6 \\
    \end{pmatrix}

    =

    \begin{pmatrix}
    w_1x_1+w_2x_2 & w_3x_1+w_4x_2 & w_5x_1+w_6x_2
    \end{pmatrix}
$$

$$
\text{np.dot(X, W)}
$$

恒等函数(y = x)

> 输出层所用的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元分类问题可以使用sigmoid函数， 多元分类问题可以使用softmax函数。

机器学习问题分类:

> 分类问题是数据属于哪一个类别的问题。

> 回归问题是根据某个输入预测一个(连续的)数值的问题。

### softmax 函数

> softmax 函数的输出是 0.0 到 1.0 之间的实数

> softmax 函数的输出值的总和是 1

> softmax 函数的输出可以理解为 概率

### 输出层神经元的数量

如果是分类问题, 输出层神经元的数量设置为类别数量

保存训练好的权重数据.

以批处理的方式输入数据.

> 输入数据的集合称为批。通过以批为单位进行推理处理，能够实现高速的运算

---

## 4. 神经网络的学习

### 从数据中学习

> 在计算机视觉领域，常用的特征量包括SIFT、SURF和HOG等。

> “特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的 转换器

SVM, KNN 分类器

与机器学习不同, 特征量也由神经网络学习.

> 深度学习有时也称为端到端机器学习(end-to-end machine  learning), 从原始数据(输入)中获得目标结果(输出)

训练数据(监督数据), 测试数据.

追求模型的泛化能力, 避免过拟合.


### 损失函数

损失函数（loss function）, 用于衡量神经网络的性能指标. 均方误差（mean squared error）, 交叉熵误差（cross entropy error）.

> 将正确解标签表示为 1，其他标签表示为 0 的表示方法称 为 one-hot 表示。

损失函数的值越小, 越接近监督数据.

### mini-batch

mini-batch学习, 从训练数据中选出一批数据(mini-batch), 对每一批数据进行学习.



---

